{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD different random seeds\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On statefulness\n",
    "\n",
    "Making a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.\n",
    "\n",
    "When using stateful RNNs, it is therefore assumed that:\n",
    "\n",
    "- all batches have the same number of samples\n",
    "- if X1 and X2 are successive batches of samples, then X2[i] is the follow-up sequence to X1[i], for every i.\n",
    "\n",
    "Notes that the methods predict, fit, train_on_batch, predict_classes, etc. will all update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X  # this is our input data, of shape (32, 21, 16)\n",
    "# we will feed it to our model in sequences of length 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(10, 16), batch_size=32, stateful=True))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# we train the network to predict the 11th timestep given the first 10:\n",
    "model.train_on_batch(X[:, :10, :], np.reshape(X[:, 10, :], (32, 16)))\n",
    "\n",
    "# the state of the network has changed. We can feed the follow-up sequences:\n",
    "model.train_on_batch(X[:, 10:20, :], np.reshape(X[:, 20, :], (32, 16)))\n",
    "\n",
    "# let's reset the states of the LSTM layer:\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whether to use LSTM or MLP\n",
    "use_LSTM = True\n",
    "\n",
    "# number of features used in the regression (for MLP)\n",
    "mlp_num_features = 10\n",
    "#\n",
    "\n",
    "# predict several timesteps at once\n",
    "lstm_predict_sequences = True\n",
    "lstm_num_predictions = 5\n",
    "\n",
    "# lstm_num_timesteps\n",
    "lstm_num_timesteps = 5\n",
    "# lstm_num_features\n",
    "lstm_num_features = 1\n",
    "# stateful?\n",
    "lstm_stateful = False\n",
    "# use two lstm layers?\n",
    "lstm_stack_layers = False\n",
    "\n",
    "# window_size\n",
    "window_size = lstm_num_timesteps if use_LSTM else mlp_num_features\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 500\n",
    "# dimensionality of the output space\n",
    "num_neurons = 4\n",
    "\n",
    "# scale the dataset to values between scale_min and scale_max\n",
    "scale = False\n",
    "scale_min = -1\n",
    "scale_max = 1\n",
    "#scaler = MinMaxScaler(feature_range=(scale_min, scale_max))\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# various test datasets\n",
    "ts_train_lineartrend = np.arange(1,101, dtype='float64').reshape(-1,1)  \n",
    "ts_test_lineartrend_outofrange = np.arange(101,121, dtype='float64').reshape(-1,1)\n",
    "ts_test_lineartrend_withinrange = np.arange(21,41, dtype='float64').reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testname = 'linear_trend_within_range'\n",
    "ts_train = ts_train_lineartrend\n",
    "ts_test = ts_test_lineartrend_withinrange\n",
    "ts_all = np.append(ts_train, ts_test).reshape(-1,1)\n",
    "len_overall = len(ts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_all.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 1), (20, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_train.shape, ts_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.],\n",
       "       [  2.],\n",
       "       [  3.],\n",
       "       [  4.],\n",
       "       [  5.],\n",
       "       [  6.],\n",
       "       [  7.],\n",
       "       [  8.],\n",
       "       [  9.],\n",
       "       [ 10.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.],\n",
       "       [ 22.],\n",
       "       [ 23.],\n",
       "       [ 24.],\n",
       "       [ 25.],\n",
       "       [ 26.],\n",
       "       [ 27.],\n",
       "       [ 28.],\n",
       "       [ 29.],\n",
       "       [ 30.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if scale:\n",
    "    ts_train = scaler.fit_transform(ts_train)\n",
    "    ts_test = scaler.transform(ts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.],\n",
       "       [  2.],\n",
       "       [  3.],\n",
       "       [  4.],\n",
       "       [  5.],\n",
       "       [  6.],\n",
       "       [  7.],\n",
       "       [  8.],\n",
       "       [  9.],\n",
       "       [ 10.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.],\n",
       "       [ 22.],\n",
       "       [ 23.],\n",
       "       [ 24.],\n",
       "       [ 25.],\n",
       "       [ 26.],\n",
       "       [ 27.],\n",
       "       [ 28.],\n",
       "       [ 29.],\n",
       "       [ 30.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, window_size):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        a = dataset[i:(i + window_size), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + window_size, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset2(dataset, window_size):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - 2 * window_size):\n",
    "        a = dataset[i:(i + window_size), 0]\n",
    "        #print(a)\n",
    "        dataX.append(a)\n",
    "        b = dataset[(i + window_size):(i + 2* window_size), 0]\n",
    "        #print(b)\n",
    "        dataY.append(b)\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 5), (90, 5), (10, 5), (10, 5))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_LSTM:\n",
    "    if lstm_predict_sequences:\n",
    "        X_train, y_train = create_dataset2(ts_train, lstm_num_timesteps)\n",
    "        X_test, y_test = create_dataset2(ts_test, lstm_num_timesteps)\n",
    "    else:    \n",
    "        X_train, y_train = create_dataset(ts_train, lstm_num_timesteps)\n",
    "        X_test, y_test = create_dataset(ts_test, lstm_num_timesteps)\n",
    "else:\n",
    "    X_train, y_train = create_dataset(ts_train, mlp_num_features)\n",
    "    X_test, y_test = create_dataset(ts_test, mlp_num_features)\n",
    "    \n",
    "# the train and test matrices end up shorter than the respective timeseries by window_size + 1!\n",
    "X_train.shape, y_train.shape, X_test.shape,  y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  5.],\n",
       "       [ 2.,  3.,  4.,  5.,  6.],\n",
       "       [ 3.,  4.,  5.,  6.,  7.],\n",
       "       [ 4.,  5.,  6.,  7.,  8.],\n",
       "       [ 5.,  6.,  7.,  8.,  9.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.,   7.,   8.,   9.,  10.],\n",
       "       [  7.,   8.,   9.,  10.,  11.],\n",
       "       [  8.,   9.,  10.,  11.,  12.],\n",
       "       [  9.,  10.,  11.,  12.,  13.],\n",
       "       [ 10.,  11.,  12.,  13.,  14.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.,  22.,  23.,  24.,  25.],\n",
       "       [ 22.,  23.,  24.,  25.,  26.],\n",
       "       [ 23.,  24.,  25.,  26.,  27.],\n",
       "       [ 24.,  25.,  26.,  27.,  28.],\n",
       "       [ 25.,  26.,  27.,  28.,  29.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 26.,  27.,  28.,  29.,  30.],\n",
       "       [ 27.,  28.,  29.,  30.,  31.],\n",
       "       [ 28.,  29.,  30.,  31.,  32.],\n",
       "       [ 29.,  30.,  31.,  32.,  33.],\n",
       "       [ 30.,  31.,  32.,  33.,  34.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_LSTM:\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "    \n",
    "    if lstm_predict_sequences:\n",
    "        y_train = np.reshape(y_train, (y_train.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "        y_test = np.reshape(y_test, (y_test.shape[0], lstm_num_predictions, lstm_num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 5, 1), (90, 5, 1), (10, 5, 1), (10, 5, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape,  y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "stateless\n",
      "predict_sequences\n",
      "(1, 5, 4)\n",
      "(1, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# LSTM input shape\n",
    "# (samples, time steps, features)\n",
    "# LSTM output shape\n",
    "# if return_sequences: 3D tensor with shape (batch_size, timesteps, units).\n",
    "# else, 2D tensor with shape (batch_size, units).\n",
    "\n",
    "if use_LSTM:\n",
    "    \n",
    "    print('LSTM')\n",
    "    # the last state for each sample at index i in a batch will be used as initial state\n",
    "    # for the sample of index i in the following batch\n",
    "    if lstm_stateful:\n",
    "        print('stateful')\n",
    "        #\n",
    "        if lstm_stack_layers:\n",
    "            print('stack_layers')\n",
    "            model.add(LSTM(num_neurons,\n",
    "                       batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]),\n",
    "                       stateful = True,\n",
    "                       return_sequences = True))\n",
    "            print(model.output_shape)\n",
    "            model.add(LSTM(num_neurons,\n",
    "                       stateful = True))\n",
    "            print(model.output_shape)\n",
    "            model.add(Dense(1))\n",
    "            print(model.output_shape)\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            \n",
    "        # \n",
    "        elif lstm_predict_sequences:\n",
    "            print('predict_sequences')\n",
    "            model.add(LSTM(num_neurons,\n",
    "                       batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]),\n",
    "                       stateful = True,\n",
    "                       return_sequences = True))\n",
    "            print(model.output_shape)\n",
    "            model.add(TimeDistributed(Dense(1)))\n",
    "            print(model.output_shape)\n",
    "            model.add(Activation(\"linear\"))  \n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            \n",
    "        #    \n",
    "        else:\n",
    "            print('predict single')\n",
    "            model.add(LSTM(num_neurons,\n",
    "                       batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]),\n",
    "                       stateful = True))\n",
    "            print(model.output_shape)\n",
    "            model.add(Dense(1))\n",
    "            print(model.output_shape)\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        \n",
    "    \n",
    "\n",
    "    # stateful == False    \n",
    "    else: \n",
    "        print('stateless')\n",
    "        \n",
    "        if lstm_stack_layers:\n",
    "            print('stack layers')\n",
    "            # input_dim: dimensionality of the input (alternatively, input_shape)\n",
    "            # required when using this layer as the first layer in a model\n",
    "            model.add(LSTM(num_neurons, input_dim = lstm_num_features, return_sequences = True))\n",
    "            print(model.output_shape)\n",
    "            model.add(LSTM(num_neurons))\n",
    "            print(model.output_shape)\n",
    "            model.add(Dense(1))\n",
    "            print(model.output_shape)\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        # \n",
    "        # \n",
    "        elif lstm_predict_sequences:\n",
    "            print('predict_sequences')\n",
    "            model.add(LSTM(num_neurons,\n",
    "                      #  input_dim = lstm_num_features,\n",
    "                       batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]), \n",
    "                       return_sequences = True))\n",
    "            print(model.output_shape) \n",
    "            model.add(TimeDistributed(Dense(1)))\n",
    "            print(model.output_shape) \n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            \n",
    "        else:\n",
    "            print('predict single')\n",
    "            model.add(LSTM(num_neurons, input_dim = lstm_num_features))\n",
    "            print(model.output_shape) \n",
    "            model.add(Dense(1))\n",
    "            print(model.output_shape) \n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        \n",
    "   \n",
    "\n",
    "# feedforward\n",
    "else:\n",
    "    print('MLP')\n",
    "    \n",
    "    model.add(Dense(num_neurons, input_dim = mlp_num_features, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (1, 5, 4)                 96        \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (1, 5, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 101.0\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "90/90 [==============================] - 0s - loss: 3374.4669     \n",
      "Epoch 2/500\n",
      "90/90 [==============================] - 0s - loss: 3317.5146     \n",
      "Epoch 3/500\n",
      "90/90 [==============================] - 0s - loss: 3288.9500     \n",
      "Epoch 4/500\n",
      "90/90 [==============================] - 0s - loss: 3263.7686     \n",
      "Epoch 5/500\n",
      "90/90 [==============================] - 0s - loss: 3239.5281     \n",
      "Epoch 6/500\n",
      "90/90 [==============================] - 0s - loss: 3215.6521     \n",
      "Epoch 7/500\n",
      "90/90 [==============================] - 0s - loss: 3174.5908     \n",
      "Epoch 8/500\n",
      "90/90 [==============================] - 0s - loss: 3118.5414     \n",
      "Epoch 9/500\n",
      "90/90 [==============================] - 0s - loss: 3083.1171     \n",
      "Epoch 10/500\n",
      "90/90 [==============================] - 0s - loss: 3050.5697     \n",
      "Epoch 11/500\n",
      "90/90 [==============================] - 0s - loss: 3012.3502     \n",
      "Epoch 12/500\n",
      "90/90 [==============================] - 0s - loss: 2967.8467     \n",
      "Epoch 13/500\n",
      "90/90 [==============================] - 0s - loss: 2935.9662     \n",
      "Epoch 14/500\n",
      "90/90 [==============================] - 0s - loss: 2904.9768     \n",
      "Epoch 15/500\n",
      "90/90 [==============================] - 0s - loss: 2874.9630     \n",
      "Epoch 16/500\n",
      "90/90 [==============================] - 0s - loss: 2845.5341     \n",
      "Epoch 17/500\n",
      "90/90 [==============================] - 0s - loss: 2816.7906     \n",
      "Epoch 18/500\n",
      "90/90 [==============================] - 0s - loss: 2788.4502     \n",
      "Epoch 19/500\n",
      "90/90 [==============================] - 0s - loss: 2760.7044     \n",
      "Epoch 20/500\n",
      "90/90 [==============================] - 0s - loss: 2733.2500     \n",
      "Epoch 21/500\n",
      "90/90 [==============================] - 0s - loss: 2706.2995     \n",
      "Epoch 22/500\n",
      "90/90 [==============================] - 0s - loss: 2679.4783     \n",
      "Epoch 23/500\n",
      "90/90 [==============================] - 0s - loss: 2653.1875     \n",
      "Epoch 24/500\n",
      "90/90 [==============================] - 0s - loss: 2627.0147     \n",
      "Epoch 25/500\n",
      "90/90 [==============================] - 0s - loss: 2601.2937     \n",
      "Epoch 26/500\n",
      "90/90 [==============================] - 0s - loss: 2575.7941     \n",
      "Epoch 27/500\n",
      "90/90 [==============================] - 0s - loss: 2550.6081     \n",
      "Epoch 28/500\n",
      "90/90 [==============================] - 0s - loss: 2525.7087     \n",
      "Epoch 29/500\n",
      "90/90 [==============================] - 0s - loss: 2501.0746     \n",
      "Epoch 30/500\n",
      "90/90 [==============================] - 0s - loss: 2476.8406     \n",
      "Epoch 31/500\n",
      "90/90 [==============================] - 0s - loss: 2452.5411     \n",
      "Epoch 32/500\n",
      "90/90 [==============================] - 0s - loss: 2428.7033     \n",
      "Epoch 33/500\n",
      "90/90 [==============================] - 0s - loss: 2405.1473     \n",
      "Epoch 34/500\n",
      "90/90 [==============================] - 0s - loss: 2381.6710     \n",
      "Epoch 35/500\n",
      "90/90 [==============================] - 0s - loss: 2358.6692     \n",
      "Epoch 36/500\n",
      "90/90 [==============================] - 0s - loss: 2335.6369     \n",
      "Epoch 37/500\n",
      "90/90 [==============================] - 0s - loss: 2312.9996     \n",
      "Epoch 38/500\n",
      "90/90 [==============================] - 0s - loss: 2290.5441     \n",
      "Epoch 39/500\n",
      "90/90 [==============================] - 0s - loss: 2268.2681     \n",
      "Epoch 40/500\n",
      "90/90 [==============================] - 0s - loss: 2246.1878     \n",
      "Epoch 41/500\n",
      "90/90 [==============================] - 0s - loss: 2224.2598     \n",
      "Epoch 42/500\n",
      "90/90 [==============================] - 0s - loss: 2202.6006     \n",
      "Epoch 43/500\n",
      "90/90 [==============================] - 0s - loss: 2181.1826     \n",
      "Epoch 44/500\n",
      "90/90 [==============================] - 0s - loss: 2159.7730     \n",
      "Epoch 45/500\n",
      "90/90 [==============================] - 0s - loss: 2138.7867     \n",
      "Epoch 46/500\n",
      "90/90 [==============================] - 0s - loss: 2117.7304     \n",
      "Epoch 47/500\n",
      "90/90 [==============================] - 0s - loss: 2097.0660     \n",
      "Epoch 48/500\n",
      "90/90 [==============================] - 0s - loss: 2076.3364     \n",
      "Epoch 49/500\n",
      "90/90 [==============================] - 0s - loss: 2055.7770     \n",
      "Epoch 50/500\n",
      "90/90 [==============================] - 0s - loss: 2035.5824     \n",
      "Epoch 51/500\n",
      "90/90 [==============================] - 0s - loss: 2015.4437     \n",
      "Epoch 52/500\n",
      "90/90 [==============================] - 0s - loss: 1995.5338     \n",
      "Epoch 53/500\n",
      "90/90 [==============================] - 0s - loss: 1975.7762     \n",
      "Epoch 54/500\n",
      "90/90 [==============================] - 0s - loss: 1956.2445     \n",
      "Epoch 55/500\n",
      "90/90 [==============================] - 0s - loss: 1936.7821     \n",
      "Epoch 56/500\n",
      "90/90 [==============================] - 0s - loss: 1917.6362     \n",
      "Epoch 57/500\n",
      "90/90 [==============================] - 0s - loss: 1898.5128     \n",
      "Epoch 58/500\n",
      "90/90 [==============================] - 0s - loss: 1879.6841     \n",
      "Epoch 59/500\n",
      "90/90 [==============================] - 0s - loss: 1860.8931     \n",
      "Epoch 60/500\n",
      "90/90 [==============================] - 0s - loss: 1842.3839     \n",
      "Epoch 61/500\n",
      "90/90 [==============================] - 0s - loss: 1824.0266     \n",
      "Epoch 62/500\n",
      "90/90 [==============================] - 0s - loss: 1805.7339     \n",
      "Epoch 63/500\n",
      "90/90 [==============================] - 0s - loss: 1787.7097     \n",
      "Epoch 64/500\n",
      "90/90 [==============================] - 0s - loss: 1769.8130     \n",
      "Epoch 65/500\n",
      "90/90 [==============================] - 0s - loss: 1751.9565     \n",
      "Epoch 66/500\n",
      "90/90 [==============================] - 0s - loss: 1734.4192     \n",
      "Epoch 67/500\n",
      "90/90 [==============================] - 0s - loss: 1716.9557     \n",
      "Epoch 68/500\n",
      "90/90 [==============================] - 0s - loss: 1699.6686     \n",
      "Epoch 69/500\n",
      "90/90 [==============================] - 0s - loss: 1682.4245     \n",
      "Epoch 70/500\n",
      "90/90 [==============================] - 0s - loss: 1665.4448     \n",
      "Epoch 71/500\n",
      "90/90 [==============================] - 0s - loss: 1648.6167     \n",
      "Epoch 72/500\n",
      "90/90 [==============================] - 0s - loss: 1631.8791     \n",
      "Epoch 73/500\n",
      "90/90 [==============================] - 0s - loss: 1615.3246     \n",
      "Epoch 74/500\n",
      "90/90 [==============================] - 0s - loss: 1598.8675     \n",
      "Epoch 75/500\n",
      "90/90 [==============================] - 0s - loss: 1582.5190     \n",
      "Epoch 76/500\n",
      "90/90 [==============================] - 0s - loss: 1566.3894     \n",
      "Epoch 77/500\n",
      "90/90 [==============================] - 0s - loss: 1550.4652     \n",
      "Epoch 78/500\n",
      "90/90 [==============================] - 0s - loss: 1534.4684     \n",
      "Epoch 79/500\n",
      "90/90 [==============================] - 0s - loss: 1518.8387     \n",
      "Epoch 80/500\n",
      "90/90 [==============================] - 0s - loss: 1503.2601     \n",
      "Epoch 81/500\n",
      "90/90 [==============================] - 0s - loss: 1487.7784     \n",
      "Epoch 82/500\n",
      "90/90 [==============================] - 0s - loss: 1472.4476     \n",
      "Epoch 83/500\n",
      "90/90 [==============================] - 0s - loss: 1457.4809     \n",
      "Epoch 84/500\n",
      "90/90 [==============================] - 0s - loss: 1442.3078     \n",
      "Epoch 85/500\n",
      "90/90 [==============================] - 0s - loss: 1427.4362     \n",
      "Epoch 86/500\n",
      "90/90 [==============================] - 0s - loss: 1412.6280     \n",
      "Epoch 87/500\n",
      "90/90 [==============================] - 0s - loss: 1398.0086     \n",
      "Epoch 88/500\n",
      "90/90 [==============================] - 0s - loss: 1383.4860     \n",
      "Epoch 89/500\n",
      "90/90 [==============================] - 0s - loss: 1369.0217     \n",
      "Epoch 90/500\n",
      "90/90 [==============================] - 0s - loss: 1354.7130     \n",
      "Epoch 91/500\n",
      "90/90 [==============================] - 0s - loss: 1340.6224     \n",
      "Epoch 92/500\n",
      "90/90 [==============================] - 0s - loss: 1326.5705     \n",
      "Epoch 93/500\n",
      "90/90 [==============================] - 0s - loss: 1312.6169     \n",
      "Epoch 94/500\n",
      "90/90 [==============================] - 0s - loss: 1298.9094     \n",
      "Epoch 95/500\n",
      "90/90 [==============================] - 0s - loss: 1285.2000     \n",
      "Epoch 96/500\n",
      "90/90 [==============================] - 0s - loss: 1271.7171     \n",
      "Epoch 97/500\n",
      "90/90 [==============================] - 0s - loss: 1258.2703     \n",
      "Epoch 98/500\n",
      "90/90 [==============================] - 0s - loss: 1244.9506     \n",
      "Epoch 99/500\n",
      "90/90 [==============================] - 0s - loss: 1231.9254     \n",
      "Epoch 100/500\n",
      "90/90 [==============================] - 0s - loss: 1218.7790     \n",
      "Epoch 101/500\n",
      "90/90 [==============================] - 0s - loss: 1205.8582     \n",
      "Epoch 102/500\n",
      "90/90 [==============================] - 0s - loss: 1193.0535     \n",
      "Epoch 103/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s - loss: 1180.3296     \n",
      "Epoch 104/500\n",
      "90/90 [==============================] - 0s - loss: 1167.6686     \n",
      "Epoch 105/500\n",
      "90/90 [==============================] - 0s - loss: 1155.2687     \n",
      "Epoch 106/500\n",
      "90/90 [==============================] - 0s - loss: 1142.9160     \n",
      "Epoch 107/500\n",
      "90/90 [==============================] - 0s - loss: 1130.6791     \n",
      "Epoch 108/500\n",
      "90/90 [==============================] - 0s - loss: 1118.5391     \n",
      "Epoch 109/500\n",
      "90/90 [==============================] - 0s - loss: 1106.5549     \n",
      "Epoch 110/500\n",
      "90/90 [==============================] - 0s - loss: 1094.5436     \n",
      "Epoch 111/500\n",
      "90/90 [==============================] - 0s - loss: 1082.8524     \n",
      "Epoch 112/500\n",
      "90/90 [==============================] - 0s - loss: 1071.1592     \n",
      "Epoch 113/500\n",
      "90/90 [==============================] - 0s - loss: 1059.5869     \n",
      "Epoch 114/500\n",
      "90/90 [==============================] - 0s - loss: 1048.2096     \n",
      "Epoch 115/500\n",
      "90/90 [==============================] - 0s - loss: 1036.7056    \n",
      "Epoch 116/500\n",
      "90/90 [==============================] - 0s - loss: 1025.5613     \n",
      "Epoch 117/500\n",
      "90/90 [==============================] - 0s - loss: 1014.3323     \n",
      "Epoch 118/500\n",
      "90/90 [==============================] - 0s - loss: 1003.2595    \n",
      "Epoch 119/500\n",
      "90/90 [==============================] - 0s - loss: 992.3579      \n",
      "Epoch 120/500\n",
      "90/90 [==============================] - 0s - loss: 981.4664     \n",
      "Epoch 121/500\n",
      "90/90 [==============================] - 0s - loss: 970.7462      \n",
      "Epoch 122/500\n",
      "90/90 [==============================] - 0s - loss: 960.0231     \n",
      "Epoch 123/500\n",
      "90/90 [==============================] - 0s - loss: 949.6110      \n",
      "Epoch 124/500\n",
      "90/90 [==============================] - 0s - loss: 939.0463     \n",
      "Epoch 125/500\n",
      "90/90 [==============================] - 0s - loss: 928.7464     \n",
      "Epoch 126/500\n",
      "90/90 [==============================] - 0s - loss: 918.5048     \n",
      "Epoch 127/500\n",
      "90/90 [==============================] - 0s - loss: 908.3479     \n",
      "Epoch 128/500\n",
      "90/90 [==============================] - 0s - loss: 898.2671     \n",
      "Epoch 129/500\n",
      "90/90 [==============================] - 0s - loss: 888.3931     \n",
      "Epoch 130/500\n",
      "90/90 [==============================] - 0s - loss: 878.3434     \n",
      "Epoch 131/500\n",
      "90/90 [==============================] - 0s - loss: 868.7428     \n",
      "Epoch 132/500\n",
      "90/90 [==============================] - 0s - loss: 859.0092     \n",
      "Epoch 133/500\n",
      "90/90 [==============================] - 0s - loss: 849.3532     \n",
      "Epoch 134/500\n",
      "90/90 [==============================] - 0s - loss: 839.9217     \n",
      "Epoch 135/500\n",
      "90/90 [==============================] - 0s - loss: 830.4872     \n",
      "Epoch 136/500\n",
      "90/90 [==============================] - 0s - loss: 821.3181     \n",
      "Epoch 137/500\n",
      "90/90 [==============================] - 0s - loss: 811.9646     \n",
      "Epoch 138/500\n",
      "90/90 [==============================] - 0s - loss: 802.7584     \n",
      "Epoch 139/500\n",
      "90/90 [==============================] - 0s - loss: 793.7609     \n",
      "Epoch 140/500\n",
      "90/90 [==============================] - 0s - loss: 784.8658     \n",
      "Epoch 141/500\n",
      "90/90 [==============================] - 0s - loss: 775.9195     \n",
      "Epoch 142/500\n",
      "90/90 [==============================] - 0s - loss: 767.1296     \n",
      "Epoch 143/500\n",
      "90/90 [==============================] - 0s - loss: 758.3753     \n",
      "Epoch 144/500\n",
      "90/90 [==============================] - 0s - loss: 749.6784     \n",
      "Epoch 145/500\n",
      "90/90 [==============================] - 0s - loss: 741.2146     \n",
      "Epoch 146/500\n",
      "90/90 [==============================] - 0s - loss: 732.7138     \n",
      "Epoch 147/500\n",
      "90/90 [==============================] - 0s - loss: 724.3040     \n",
      "Epoch 148/500\n",
      "90/90 [==============================] - 0s - loss: 716.0774     \n",
      "Epoch 149/500\n",
      "90/90 [==============================] - 0s - loss: 707.7851     \n",
      "Epoch 150/500\n",
      "90/90 [==============================] - 0s - loss: 699.6966     \n",
      "Epoch 151/500\n",
      "90/90 [==============================] - 0s - loss: 691.6423     \n",
      "Epoch 152/500\n",
      "90/90 [==============================] - 0s - loss: 683.6488     \n",
      "Epoch 153/500\n",
      "90/90 [==============================] - 0s - loss: 675.7769     \n",
      "Epoch 154/500\n",
      "90/90 [==============================] - 0s - loss: 667.8970     \n",
      "Epoch 155/500\n",
      "90/90 [==============================] - 0s - loss: 660.2643     \n",
      "Epoch 156/500\n",
      "90/90 [==============================] - 0s - loss: 652.4971     \n",
      "Epoch 157/500\n",
      "90/90 [==============================] - 0s - loss: 644.9968     \n",
      "Epoch 158/500\n",
      "90/90 [==============================] - 0s - loss: 637.4401     \n",
      "Epoch 159/500\n",
      "90/90 [==============================] - 0s - loss: 629.9719     \n",
      "Epoch 160/500\n",
      "90/90 [==============================] - 0s - loss: 622.5961     \n",
      "Epoch 161/500\n",
      "90/90 [==============================] - 0s - loss: 615.3490     \n",
      "Epoch 162/500\n",
      "90/90 [==============================] - 0s - loss: 608.1000     \n",
      "Epoch 163/500\n",
      "90/90 [==============================] - 0s - loss: 601.1098     \n",
      "Epoch 164/500\n",
      "90/90 [==============================] - 0s - loss: 593.9497     \n",
      "Epoch 165/500\n",
      "90/90 [==============================] - 0s - loss: 586.8986     \n",
      "Epoch 166/500\n",
      "90/90 [==============================] - 0s - loss: 579.9477     \n",
      "Epoch 167/500\n",
      "90/90 [==============================] - 0s - loss: 573.2470     \n",
      "Epoch 168/500\n",
      "90/90 [==============================] - 0s - loss: 566.3661     \n",
      "Epoch 169/500\n",
      "90/90 [==============================] - 0s - loss: 559.5998     \n",
      "Epoch 170/500\n",
      "90/90 [==============================] - 0s - loss: 552.9741     \n",
      "Epoch 171/500\n",
      "90/90 [==============================] - 0s - loss: 546.3585     \n",
      "Epoch 172/500\n",
      "90/90 [==============================] - 0s - loss: 539.9210     \n",
      "Epoch 173/500\n",
      "90/90 [==============================] - 0s - loss: 533.4641     \n",
      "Epoch 174/500\n",
      "90/90 [==============================] - 0s - loss: 527.1722     \n",
      "Epoch 175/500\n",
      "90/90 [==============================] - 0s - loss: 520.7555     \n",
      "Epoch 176/500\n",
      "90/90 [==============================] - 0s - loss: 514.5051     \n",
      "Epoch 177/500\n",
      "90/90 [==============================] - 0s - loss: 508.2867     \n",
      "Epoch 178/500\n",
      "90/90 [==============================] - 0s - loss: 502.1337     \n",
      "Epoch 179/500\n",
      "90/90 [==============================] - 0s - loss: 496.1629     \n",
      "Epoch 180/500\n",
      "90/90 [==============================] - 0s - loss: 490.0791     \n",
      "Epoch 181/500\n",
      "90/90 [==============================] - 0s - loss: 484.1680     \n",
      "Epoch 182/500\n",
      "90/90 [==============================] - 0s - loss: 478.3610     \n",
      "Epoch 183/500\n",
      "90/90 [==============================] - 0s - loss: 472.5835     \n",
      "Epoch 184/500\n",
      "90/90 [==============================] - 0s - loss: 466.7233     \n",
      "Epoch 185/500\n",
      "90/90 [==============================] - 0s - loss: 461.0388     \n",
      "Epoch 186/500\n",
      "90/90 [==============================] - 0s - loss: 455.3949     \n",
      "Epoch 187/500\n",
      "90/90 [==============================] - 0s - loss: 449.8503     \n",
      "Epoch 188/500\n",
      "90/90 [==============================] - 0s - loss: 444.2954     \n",
      "Epoch 189/500\n",
      "90/90 [==============================] - 0s - loss: 438.8495     \n",
      "Epoch 190/500\n",
      "90/90 [==============================] - 0s - loss: 433.4214     \n",
      "Epoch 191/500\n",
      "90/90 [==============================] - 0s - loss: 428.1113     \n",
      "Epoch 192/500\n",
      "90/90 [==============================] - 0s - loss: 422.7909     \n",
      "Epoch 193/500\n",
      "90/90 [==============================] - 0s - loss: 417.5770     \n",
      "Epoch 194/500\n",
      "90/90 [==============================] - 0s - loss: 412.4614     \n",
      "Epoch 195/500\n",
      "90/90 [==============================] - 0s - loss: 407.2824     \n",
      "Epoch 196/500\n",
      "90/90 [==============================] - 0s - loss: 402.2103     \n",
      "Epoch 197/500\n",
      "90/90 [==============================] - 0s - loss: 397.2056     \n",
      "Epoch 198/500\n",
      "90/90 [==============================] - 0s - loss: 392.2734     \n",
      "Epoch 199/500\n",
      "90/90 [==============================] - 0s - loss: 387.2950     \n",
      "Epoch 200/500\n",
      "90/90 [==============================] - 0s - loss: 382.4471     \n",
      "Epoch 201/500\n",
      "90/90 [==============================] - 0s - loss: 377.6956     \n",
      "Epoch 202/500\n",
      "90/90 [==============================] - 0s - loss: 372.9028     \n",
      "Epoch 203/500\n",
      "90/90 [==============================] - 0s - loss: 368.3454     \n",
      "Epoch 204/500\n",
      "90/90 [==============================] - 0s - loss: 363.8210     \n",
      "Epoch 205/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s - loss: 359.0157     \n",
      "Epoch 206/500\n",
      "90/90 [==============================] - 0s - loss: 354.4918     \n",
      "Epoch 207/500\n",
      "90/90 [==============================] - 0s - loss: 349.9070     \n",
      "Epoch 208/500\n",
      "90/90 [==============================] - 0s - loss: 345.5274     \n",
      "Epoch 209/500\n",
      "90/90 [==============================] - 0s - loss: 341.3036     \n",
      "Epoch 210/500\n",
      "90/90 [==============================] - 0s - loss: 336.9153     \n",
      "Epoch 211/500\n",
      "90/90 [==============================] - 0s - loss: 332.4666     \n",
      "Epoch 212/500\n",
      "90/90 [==============================] - 0s - loss: 328.1586     \n",
      "Epoch 213/500\n",
      "90/90 [==============================] - 0s - loss: 324.0375     \n",
      "Epoch 214/500\n",
      "90/90 [==============================] - 0s - loss: 319.8473     \n",
      "Epoch 215/500\n",
      "90/90 [==============================] - 0s - loss: 315.6953     \n",
      "Epoch 216/500\n",
      "90/90 [==============================] - 0s - loss: 311.6634     \n",
      "Epoch 217/500\n",
      "90/90 [==============================] - 0s - loss: 307.6557     \n",
      "Epoch 218/500\n",
      "90/90 [==============================] - 0s - loss: 303.6637     \n",
      "Epoch 219/500\n",
      "90/90 [==============================] - 0s - loss: 299.7065     \n",
      "Epoch 220/500\n",
      "90/90 [==============================] - 0s - loss: 296.2248     \n",
      "Epoch 221/500\n",
      "90/90 [==============================] - 0s - loss: 292.1002     \n",
      "Epoch 222/500\n",
      "90/90 [==============================] - 0s - loss: 288.1954     \n",
      "Epoch 223/500\n",
      "90/90 [==============================] - 0s - loss: 284.4020     \n",
      "Epoch 224/500\n",
      "90/90 [==============================] - 0s - loss: 280.7272     \n",
      "Epoch 225/500\n",
      "90/90 [==============================] - 0s - loss: 277.1420     \n",
      "Epoch 226/500\n",
      "90/90 [==============================] - 0s - loss: 273.4289     \n",
      "Epoch 227/500\n",
      "90/90 [==============================] - 0s - loss: 269.9481     \n",
      "Epoch 228/500\n",
      "90/90 [==============================] - 0s - loss: 266.3294     \n",
      "Epoch 229/500\n",
      "90/90 [==============================] - 0s - loss: 263.0432     \n",
      "Epoch 230/500\n",
      "90/90 [==============================] - 0s - loss: 259.5209     \n",
      "Epoch 231/500\n",
      "90/90 [==============================] - 0s - loss: 255.8546     \n",
      "Epoch 232/500\n",
      "90/90 [==============================] - 0s - loss: 252.4682     \n",
      "Epoch 233/500\n",
      "90/90 [==============================] - 0s - loss: 249.2229     \n",
      "Epoch 234/500\n",
      "90/90 [==============================] - 0s - loss: 245.7916     \n",
      "Epoch 235/500\n",
      "90/90 [==============================] - 0s - loss: 242.5557     \n",
      "Epoch 236/500\n",
      "90/90 [==============================] - 0s - loss: 239.3037     \n",
      "Epoch 237/500\n",
      "90/90 [==============================] - 0s - loss: 236.0949     \n",
      "Epoch 238/500\n",
      "90/90 [==============================] - 0s - loss: 232.9952     \n",
      "Epoch 239/500\n",
      "90/90 [==============================] - 0s - loss: 229.7963     \n",
      "Epoch 240/500\n",
      "90/90 [==============================] - 0s - loss: 226.7391     \n",
      "Epoch 241/500\n",
      "90/90 [==============================] - 0s - loss: 223.6802     \n",
      "Epoch 242/500\n",
      "90/90 [==============================] - 0s - loss: 220.6955     \n",
      "Epoch 243/500\n",
      "90/90 [==============================] - 0s - loss: 217.7495     \n",
      "Epoch 244/500\n",
      "90/90 [==============================] - 0s - loss: 215.1089     \n",
      "Epoch 245/500\n",
      "90/90 [==============================] - 0s - loss: 211.8782     \n",
      "Epoch 246/500\n",
      "90/90 [==============================] - 0s - loss: 208.9223     \n",
      "Epoch 247/500\n",
      "90/90 [==============================] - 0s - loss: 206.0964     \n",
      "Epoch 248/500\n",
      "90/90 [==============================] - 0s - loss: 203.3188     \n",
      "Epoch 249/500\n",
      "90/90 [==============================] - 0s - loss: 200.4983     \n",
      "Epoch 250/500\n",
      "90/90 [==============================] - 0s - loss: 197.7366     \n",
      "Epoch 251/500\n",
      "90/90 [==============================] - 0s - loss: 195.0709     \n",
      "Epoch 252/500\n",
      "90/90 [==============================] - 0s - loss: 192.4896     \n",
      "Epoch 253/500\n",
      "90/90 [==============================] - 0s - loss: 189.9619     \n",
      "Epoch 254/500\n",
      "90/90 [==============================] - 0s - loss: 187.3612     \n",
      "Epoch 255/500\n",
      "90/90 [==============================] - 0s - loss: 184.5057     \n",
      "Epoch 256/500\n",
      "90/90 [==============================] - 0s - loss: 182.0006     \n",
      "Epoch 257/500\n",
      "90/90 [==============================] - 0s - loss: 179.3975     \n",
      "Epoch 258/500\n",
      "90/90 [==============================] - 0s - loss: 176.8958     \n",
      "Epoch 259/500\n",
      "90/90 [==============================] - 0s - loss: 174.4846     \n",
      "Epoch 260/500\n",
      "90/90 [==============================] - 0s - loss: 172.0572     \n",
      "Epoch 261/500\n",
      "90/90 [==============================] - 0s - loss: 169.6195     \n",
      "Epoch 262/500\n",
      "90/90 [==============================] - 0s - loss: 167.3136     \n",
      "Epoch 263/500\n",
      "90/90 [==============================] - 0s - loss: 165.0775     \n",
      "Epoch 264/500\n",
      "90/90 [==============================] - 0s - loss: 162.6350     \n",
      "Epoch 265/500\n",
      "90/90 [==============================] - 0s - loss: 160.6112     \n",
      "Epoch 266/500\n",
      "90/90 [==============================] - 0s - loss: 158.1011     \n",
      "Epoch 267/500\n",
      "90/90 [==============================] - 0s - loss: 155.8258     \n",
      "Epoch 268/500\n",
      "90/90 [==============================] - 0s - loss: 153.6093     \n",
      "Epoch 269/500\n",
      "90/90 [==============================] - 0s - loss: 151.6015     \n",
      "Epoch 270/500\n",
      "90/90 [==============================] - 0s - loss: 149.3323     \n",
      "Epoch 271/500\n",
      "90/90 [==============================] - 0s - loss: 147.2227     \n",
      "Epoch 272/500\n",
      "90/90 [==============================] - 0s - loss: 145.0703     \n",
      "Epoch 273/500\n",
      "90/90 [==============================] - 0s - loss: 142.9840     \n",
      "Epoch 274/500\n",
      "90/90 [==============================] - 0s - loss: 141.0773     \n",
      "Epoch 275/500\n",
      "90/90 [==============================] - 0s - loss: 139.0012     \n",
      "Epoch 276/500\n",
      "90/90 [==============================] - 0s - loss: 136.9785     \n",
      "Epoch 277/500\n",
      "90/90 [==============================] - 0s - loss: 135.0100     \n",
      "Epoch 278/500\n",
      "90/90 [==============================] - 0s - loss: 133.0851     \n",
      "Epoch 279/500\n",
      "90/90 [==============================] - 0s - loss: 131.1372     \n",
      "Epoch 280/500\n",
      "90/90 [==============================] - 0s - loss: 129.2029     \n",
      "Epoch 281/500\n",
      "90/90 [==============================] - 0s - loss: 127.3615     \n",
      "Epoch 282/500\n",
      "90/90 [==============================] - 0s - loss: 125.5481     \n",
      "Epoch 283/500\n",
      "90/90 [==============================] - 0s - loss: 123.6771     \n",
      "Epoch 284/500\n",
      "90/90 [==============================] - 0s - loss: 121.8497     \n",
      "Epoch 285/500\n",
      "90/90 [==============================] - 0s - loss: 120.3640     \n",
      "Epoch 286/500\n",
      "90/90 [==============================] - 0s - loss: 118.3866     \n",
      "Epoch 287/500\n",
      "90/90 [==============================] - 0s - loss: 116.6921     \n",
      "Epoch 288/500\n",
      "90/90 [==============================] - 0s - loss: 114.9158    \n",
      "Epoch 289/500\n",
      "90/90 [==============================] - 0s - loss: 113.2530     \n",
      "Epoch 290/500\n",
      "90/90 [==============================] - 0s - loss: 111.7644     \n",
      "Epoch 291/500\n",
      "90/90 [==============================] - 0s - loss: 109.9625     \n",
      "Epoch 292/500\n",
      "90/90 [==============================] - 0s - loss: 108.3196     \n",
      "Epoch 293/500\n",
      "90/90 [==============================] - 0s - loss: 106.7188     \n",
      "Epoch 294/500\n",
      "90/90 [==============================] - 0s - loss: 105.2261     \n",
      "Epoch 295/500\n",
      "90/90 [==============================] - 0s - loss: 103.5970     \n",
      "Epoch 296/500\n",
      "90/90 [==============================] - 0s - loss: 102.0467     \n",
      "Epoch 297/500\n",
      "90/90 [==============================] - 0s - loss: 100.5412    \n",
      "Epoch 298/500\n",
      "90/90 [==============================] - 0s - loss: 99.1724     \n",
      "Epoch 299/500\n",
      "90/90 [==============================] - 0s - loss: 97.6288      \n",
      "Epoch 300/500\n",
      "90/90 [==============================] - 0s - loss: 96.0906     \n",
      "Epoch 301/500\n",
      "90/90 [==============================] - 0s - loss: 95.0811     \n",
      "Epoch 302/500\n",
      "90/90 [==============================] - 0s - loss: 93.4810     \n",
      "Epoch 303/500\n",
      "90/90 [==============================] - 0s - loss: 91.8874     \n",
      "Epoch 304/500\n",
      "90/90 [==============================] - 0s - loss: 90.4654     \n",
      "Epoch 305/500\n",
      "90/90 [==============================] - 0s - loss: 89.2130     \n",
      "Epoch 306/500\n",
      "90/90 [==============================] - 0s - loss: 87.8184     \n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s - loss: 86.4971     \n",
      "Epoch 308/500\n",
      "90/90 [==============================] - 0s - loss: 85.3629     \n",
      "Epoch 309/500\n",
      "90/90 [==============================] - 0s - loss: 83.9871     \n",
      "Epoch 310/500\n",
      "90/90 [==============================] - 0s - loss: 82.5688     \n",
      "Epoch 311/500\n",
      "90/90 [==============================] - 0s - loss: 81.5548     \n",
      "Epoch 312/500\n",
      "90/90 [==============================] - 0s - loss: 80.0724     \n",
      "Epoch 313/500\n",
      "90/90 [==============================] - 0s - loss: 78.8535     \n",
      "Epoch 314/500\n",
      "90/90 [==============================] - 0s - loss: 77.7450     \n",
      "Epoch 315/500\n",
      "90/90 [==============================] - 0s - loss: 76.4332     \n",
      "Epoch 316/500\n",
      "90/90 [==============================] - 0s - loss: 75.2576     \n",
      "Epoch 317/500\n",
      "90/90 [==============================] - 0s - loss: 74.1483     \n",
      "Epoch 318/500\n",
      "90/90 [==============================] - 0s - loss: 73.2550     \n",
      "Epoch 319/500\n",
      "90/90 [==============================] - 0s - loss: 71.9459     \n",
      "Epoch 320/500\n",
      "90/90 [==============================] - 0s - loss: 70.7968     \n",
      "Epoch 321/500\n",
      "90/90 [==============================] - 0s - loss: 69.6864     \n",
      "Epoch 322/500\n",
      "90/90 [==============================] - 0s - loss: 68.5836     \n",
      "Epoch 323/500\n",
      "90/90 [==============================] - 0s - loss: 67.4983     \n",
      "Epoch 324/500\n",
      "90/90 [==============================] - 0s - loss: 66.5424     \n",
      "Epoch 325/500\n",
      "90/90 [==============================] - 0s - loss: 65.7669     \n",
      "Epoch 326/500\n",
      "90/90 [==============================] - 0s - loss: 64.4024     \n",
      "Epoch 327/500\n",
      "90/90 [==============================] - 0s - loss: 63.4083     \n",
      "Epoch 328/500\n",
      "90/90 [==============================] - 0s - loss: 62.4725     \n",
      "Epoch 329/500\n",
      "90/90 [==============================] - 0s - loss: 61.4471     \n",
      "Epoch 330/500\n",
      "90/90 [==============================] - 0s - loss: 60.9451     \n",
      "Epoch 331/500\n",
      "90/90 [==============================] - 0s - loss: 59.6157     \n",
      "Epoch 332/500\n",
      "90/90 [==============================] - 0s - loss: 58.6236     \n",
      "Epoch 333/500\n",
      "90/90 [==============================] - 0s - loss: 57.8578     \n",
      "Epoch 334/500\n",
      "90/90 [==============================] - 0s - loss: 56.7737     \n",
      "Epoch 335/500\n",
      "90/90 [==============================] - 0s - loss: 55.8753     \n",
      "Epoch 336/500\n",
      "90/90 [==============================] - 0s - loss: 55.0249     \n",
      "Epoch 337/500\n",
      "90/90 [==============================] - 0s - loss: 54.1938     \n",
      "Epoch 338/500\n",
      "90/90 [==============================] - 0s - loss: 53.4533     \n",
      "Epoch 339/500\n",
      "90/90 [==============================] - 0s - loss: 52.4690     \n",
      "Epoch 340/500\n",
      "90/90 [==============================] - 0s - loss: 51.6782     \n",
      "Epoch 341/500\n",
      "90/90 [==============================] - 0s - loss: 50.7884     \n",
      "Epoch 342/500\n",
      "90/90 [==============================] - 0s - loss: 50.0697     \n",
      "Epoch 343/500\n",
      "90/90 [==============================] - 0s - loss: 49.2497     \n",
      "Epoch 344/500\n",
      "90/90 [==============================] - 0s - loss: 48.4618     \n",
      "Epoch 345/500\n",
      "90/90 [==============================] - 0s - loss: 47.6663     \n",
      "Epoch 346/500\n",
      "90/90 [==============================] - 0s - loss: 47.0138     \n",
      "Epoch 347/500\n",
      "90/90 [==============================] - 0s - loss: 46.1753     \n",
      "Epoch 348/500\n",
      "90/90 [==============================] - 0s - loss: 45.4543     \n",
      "Epoch 349/500\n",
      "90/90 [==============================] - 0s - loss: 44.6096     \n",
      "Epoch 350/500\n",
      "90/90 [==============================] - 0s - loss: 43.8909     \n",
      "Epoch 351/500\n",
      "90/90 [==============================] - 0s - loss: 43.2121     \n",
      "Epoch 352/500\n",
      "90/90 [==============================] - 0s - loss: 42.5580     \n",
      "Epoch 353/500\n",
      "90/90 [==============================] - 0s - loss: 41.9176     \n",
      "Epoch 354/500\n",
      "90/90 [==============================] - 0s - loss: 41.3374     \n",
      "Epoch 355/500\n",
      "90/90 [==============================] - 0s - loss: 41.2496     \n",
      "Epoch 356/500\n",
      "90/90 [==============================] - 0s - loss: 39.9406     \n",
      "Epoch 357/500\n",
      "90/90 [==============================] - 0s - loss: 39.2424     \n",
      "Epoch 358/500\n",
      "90/90 [==============================] - 0s - loss: 38.6290     \n",
      "Epoch 359/500\n",
      "90/90 [==============================] - 0s - loss: 37.9150     \n",
      "Epoch 360/500\n",
      "90/90 [==============================] - 0s - loss: 37.2640     \n",
      "Epoch 361/500\n",
      "90/90 [==============================] - 0s - loss: 36.6748     \n",
      "Epoch 362/500\n",
      "90/90 [==============================] - 0s - loss: 36.2657     \n",
      "Epoch 363/500\n",
      "90/90 [==============================] - 0s - loss: 35.5148     \n",
      "Epoch 364/500\n",
      "90/90 [==============================] - 0s - loss: 35.2172     \n",
      "Epoch 365/500\n",
      "90/90 [==============================] - 0s - loss: 34.6497     \n",
      "Epoch 366/500\n",
      "90/90 [==============================] - 0s - loss: 33.8106     \n",
      "Epoch 367/500\n",
      "90/90 [==============================] - 0s - loss: 33.2261     \n",
      "Epoch 368/500\n",
      "90/90 [==============================] - 0s - loss: 33.1418     \n",
      "Epoch 369/500\n",
      "90/90 [==============================] - 0s - loss: 32.1822     \n",
      "Epoch 370/500\n",
      "90/90 [==============================] - 0s - loss: 31.7110     \n",
      "Epoch 371/500\n",
      "90/90 [==============================] - 0s - loss: 31.1460     \n",
      "Epoch 372/500\n",
      "90/90 [==============================] - 0s - loss: 32.3432     \n",
      "Epoch 373/500\n",
      "90/90 [==============================] - 0s - loss: 30.6202     \n",
      "Epoch 374/500\n",
      "90/90 [==============================] - 0s - loss: 29.6156     \n",
      "Epoch 375/500\n",
      "90/90 [==============================] - 0s - loss: 29.1189     \n",
      "Epoch 376/500\n",
      "90/90 [==============================] - 0s - loss: 28.6492     \n",
      "Epoch 377/500\n",
      "90/90 [==============================] - 0s - loss: 28.1643     \n",
      "Epoch 378/500\n",
      "90/90 [==============================] - 0s - loss: 27.7233     \n",
      "Epoch 379/500\n",
      "90/90 [==============================] - 0s - loss: 27.3125     \n",
      "Epoch 380/500\n",
      "90/90 [==============================] - 0s - loss: 26.8891     \n",
      "Epoch 381/500\n",
      "90/90 [==============================] - 0s - loss: 26.3314     \n",
      "Epoch 382/500\n",
      "90/90 [==============================] - 0s - loss: 25.9368     \n",
      "Epoch 383/500\n",
      "90/90 [==============================] - 0s - loss: 25.4962     \n",
      "Epoch 384/500\n",
      "90/90 [==============================] - 0s - loss: 25.0996     \n",
      "Epoch 385/500\n",
      "90/90 [==============================] - 0s - loss: 24.7032     \n",
      "Epoch 386/500\n",
      "90/90 [==============================] - 0s - loss: 24.3091     \n",
      "Epoch 387/500\n",
      "90/90 [==============================] - 0s - loss: 23.8640     \n",
      "Epoch 388/500\n",
      "90/90 [==============================] - 0s - loss: 23.4829     \n",
      "Epoch 389/500\n",
      "90/90 [==============================] - 0s - loss: 23.0620     \n",
      "Epoch 390/500\n",
      "90/90 [==============================] - 0s - loss: 22.8825     \n",
      "Epoch 391/500\n",
      "90/90 [==============================] - 0s - loss: 22.8107     \n",
      "Epoch 392/500\n",
      "90/90 [==============================] - 0s - loss: 22.8270     \n",
      "Epoch 393/500\n",
      "90/90 [==============================] - 0s - loss: 21.8366     \n",
      "Epoch 394/500\n",
      "90/90 [==============================] - 0s - loss: 21.2465     \n",
      "Epoch 395/500\n",
      "90/90 [==============================] - 0s - loss: 20.9579     \n",
      "Epoch 396/500\n",
      "90/90 [==============================] - 0s - loss: 21.1394     \n",
      "Epoch 397/500\n",
      "90/90 [==============================] - 0s - loss: 20.1700     \n",
      "Epoch 398/500\n",
      "90/90 [==============================] - 0s - loss: 19.8386     \n",
      "Epoch 399/500\n",
      "90/90 [==============================] - 0s - loss: 19.9778     \n",
      "Epoch 400/500\n",
      "90/90 [==============================] - 0s - loss: 19.3258     \n",
      "Epoch 401/500\n",
      "90/90 [==============================] - 0s - loss: 18.8807     \n",
      "Epoch 402/500\n",
      "90/90 [==============================] - 0s - loss: 18.6576     \n",
      "Epoch 403/500\n",
      "90/90 [==============================] - 0s - loss: 18.2364     \n",
      "Epoch 404/500\n",
      "90/90 [==============================] - 0s - loss: 18.0609     \n",
      "Epoch 405/500\n",
      "90/90 [==============================] - 0s - loss: 17.7391     \n",
      "Epoch 406/500\n",
      "90/90 [==============================] - 0s - loss: 17.4576     \n",
      "Epoch 407/500\n",
      "90/90 [==============================] - 0s - loss: 17.0408     \n",
      "Epoch 408/500\n",
      "90/90 [==============================] - 0s - loss: 16.8186     \n",
      "Epoch 409/500\n",
      "90/90 [==============================] - 0s - loss: 16.5338     \n",
      "Epoch 410/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s - loss: 16.2264     \n",
      "Epoch 411/500\n",
      "90/90 [==============================] - 0s - loss: 15.9181     \n",
      "Epoch 412/500\n",
      "90/90 [==============================] - 0s - loss: 15.8171     \n",
      "Epoch 413/500\n",
      "90/90 [==============================] - 0s - loss: 15.5938     \n",
      "Epoch 414/500\n",
      "90/90 [==============================] - 0s - loss: 15.2729     \n",
      "Epoch 415/500\n",
      "90/90 [==============================] - 0s - loss: 15.1465     \n",
      "Epoch 416/500\n",
      "90/90 [==============================] - 0s - loss: 15.0433     \n",
      "Epoch 417/500\n",
      "90/90 [==============================] - 0s - loss: 14.6670     \n",
      "Epoch 418/500\n",
      "90/90 [==============================] - 0s - loss: 14.2541     \n",
      "Epoch 419/500\n",
      "90/90 [==============================] - 0s - loss: 14.4070     \n",
      "Epoch 420/500\n",
      "90/90 [==============================] - 0s - loss: 13.9839     \n",
      "Epoch 421/500\n",
      "90/90 [==============================] - 0s - loss: 13.5483     \n",
      "Epoch 422/500\n",
      "90/90 [==============================] - 0s - loss: 14.4667     \n",
      "Epoch 423/500\n",
      "90/90 [==============================] - 0s - loss: 13.1616     \n",
      "Epoch 424/500\n",
      "90/90 [==============================] - 0s - loss: 12.9173     \n",
      "Epoch 425/500\n",
      "90/90 [==============================] - 0s - loss: 12.7636     \n",
      "Epoch 426/500\n",
      "90/90 [==============================] - 0s - loss: 12.4549     \n",
      "Epoch 427/500\n",
      "90/90 [==============================] - 0s - loss: 12.2426     \n",
      "Epoch 428/500\n",
      "90/90 [==============================] - 0s - loss: 12.1366     \n",
      "Epoch 429/500\n",
      "90/90 [==============================] - 0s - loss: 11.8670     \n",
      "Epoch 430/500\n",
      "90/90 [==============================] - 0s - loss: 11.6685     \n",
      "Epoch 431/500\n",
      "90/90 [==============================] - 0s - loss: 11.6132     \n",
      "Epoch 432/500\n",
      "90/90 [==============================] - 0s - loss: 11.3746     \n",
      "Epoch 433/500\n",
      "90/90 [==============================] - 0s - loss: 11.1864     \n",
      "Epoch 434/500\n",
      "90/90 [==============================] - 0s - loss: 11.1476     \n",
      "Epoch 435/500\n",
      "90/90 [==============================] - 0s - loss: 11.0127     \n",
      "Epoch 436/500\n",
      "90/90 [==============================] - 0s - loss: 10.6968     \n",
      "Epoch 437/500\n",
      "90/90 [==============================] - 0s - loss: 10.6247     \n",
      "Epoch 438/500\n",
      "90/90 [==============================] - 0s - loss: 12.1640     \n",
      "Epoch 439/500\n",
      "90/90 [==============================] - 0s - loss: 10.2007     \n",
      "Epoch 440/500\n",
      "90/90 [==============================] - 0s - loss: 9.9995     \n",
      "Epoch 441/500\n",
      "90/90 [==============================] - 0s - loss: 9.7776     \n",
      "Epoch 442/500\n",
      "90/90 [==============================] - 0s - loss: 9.6159     \n",
      "Epoch 443/500\n",
      "90/90 [==============================] - 0s - loss: 9.4476     \n",
      "Epoch 444/500\n",
      "90/90 [==============================] - 0s - loss: 9.3071      \n",
      "Epoch 445/500\n",
      "90/90 [==============================] - 0s - loss: 9.1935     \n",
      "Epoch 446/500\n",
      "90/90 [==============================] - 0s - loss: 9.0174     \n",
      "Epoch 447/500\n",
      "90/90 [==============================] - 0s - loss: 8.8930     \n",
      "Epoch 448/500\n",
      "90/90 [==============================] - 0s - loss: 8.7563     \n",
      "Epoch 449/500\n",
      "90/90 [==============================] - 0s - loss: 8.6676     \n",
      "Epoch 450/500\n",
      "90/90 [==============================] - 0s - loss: 8.5213     \n",
      "Epoch 451/500\n",
      "90/90 [==============================] - 0s - loss: 8.3872     \n",
      "Epoch 452/500\n",
      "90/90 [==============================] - 0s - loss: 8.2308     \n",
      "Epoch 453/500\n",
      "90/90 [==============================] - 0s - loss: 8.1256     \n",
      "Epoch 454/500\n",
      "90/90 [==============================] - 0s - loss: 8.0112     \n",
      "Epoch 455/500\n",
      "90/90 [==============================] - 0s - loss: 8.5596     \n",
      "Epoch 456/500\n",
      "90/90 [==============================] - 0s - loss: 12.2072     \n",
      "Epoch 457/500\n",
      "90/90 [==============================] - 0s - loss: 7.9174     \n",
      "Epoch 458/500\n",
      "90/90 [==============================] - 0s - loss: 7.6014     \n",
      "Epoch 459/500\n",
      "90/90 [==============================] - 0s - loss: 7.4009     \n",
      "Epoch 460/500\n",
      "90/90 [==============================] - 0s - loss: 7.3172     \n",
      "Epoch 461/500\n",
      "90/90 [==============================] - 0s - loss: 7.2310     \n",
      "Epoch 462/500\n",
      "90/90 [==============================] - 0s - loss: 7.0436     \n",
      "Epoch 463/500\n",
      "90/90 [==============================] - 0s - loss: 6.9252     \n",
      "Epoch 464/500\n",
      "90/90 [==============================] - 0s - loss: 6.8680     \n",
      "Epoch 465/500\n",
      "90/90 [==============================] - 0s - loss: 6.7642     \n",
      "Epoch 466/500\n",
      "90/90 [==============================] - 0s - loss: 6.7711     \n",
      "Epoch 467/500\n",
      "90/90 [==============================] - 0s - loss: 6.5290     \n",
      "Epoch 468/500\n",
      "90/90 [==============================] - 0s - loss: 6.4593     \n",
      "Epoch 469/500\n",
      "90/90 [==============================] - 0s - loss: 6.3495     \n",
      "Epoch 470/500\n",
      "90/90 [==============================] - 0s - loss: 6.5254     \n",
      "Epoch 471/500\n",
      "90/90 [==============================] - 0s - loss: 6.2540     \n",
      "Epoch 472/500\n",
      "90/90 [==============================] - 0s - loss: 6.2666     \n",
      "Epoch 473/500\n",
      "90/90 [==============================] - 0s - loss: 6.1344     \n",
      "Epoch 474/500\n",
      "90/90 [==============================] - 0s - loss: 6.3026     \n",
      "Epoch 475/500\n",
      "90/90 [==============================] - 0s - loss: 6.8388     \n",
      "Epoch 476/500\n",
      "90/90 [==============================] - 0s - loss: 5.9991     \n",
      "Epoch 477/500\n",
      "90/90 [==============================] - 0s - loss: 5.6812     \n",
      "Epoch 478/500\n",
      "90/90 [==============================] - 0s - loss: 5.6260     \n",
      "Epoch 479/500\n",
      "90/90 [==============================] - 0s - loss: 5.9536     \n",
      "Epoch 480/500\n",
      "90/90 [==============================] - 0s - loss: 5.6060     \n",
      "Epoch 481/500\n",
      "90/90 [==============================] - 0s - loss: 5.4264     \n",
      "Epoch 482/500\n",
      "90/90 [==============================] - 0s - loss: 5.2768     \n",
      "Epoch 483/500\n",
      "90/90 [==============================] - 0s - loss: 5.3110     \n",
      "Epoch 484/500\n",
      "90/90 [==============================] - 0s - loss: 5.1307     \n",
      "Epoch 485/500\n",
      "90/90 [==============================] - 0s - loss: 5.1941     \n",
      "Epoch 486/500\n",
      "90/90 [==============================] - 0s - loss: 4.9922     \n",
      "Epoch 487/500\n",
      "90/90 [==============================] - 0s - loss: 4.8775     \n",
      "Epoch 488/500\n",
      "90/90 [==============================] - 0s - loss: 4.9458     \n",
      "Epoch 489/500\n",
      "90/90 [==============================] - 0s - loss: 4.8189     \n",
      "Epoch 490/500\n",
      "90/90 [==============================] - 0s - loss: 4.6925     \n",
      "Epoch 491/500\n",
      "90/90 [==============================] - 0s - loss: 4.6288     \n",
      "Epoch 492/500\n",
      "90/90 [==============================] - 0s - loss: 4.7652     \n",
      "Epoch 493/500\n",
      "90/90 [==============================] - 0s - loss: 4.5604     \n",
      "Epoch 494/500\n",
      "90/90 [==============================] - 0s - loss: 4.5724     \n",
      "Epoch 495/500\n",
      "90/90 [==============================] - 0s - loss: 4.5549     \n",
      "Epoch 496/500\n",
      "90/90 [==============================] - 0s - loss: 4.6809     \n",
      "Epoch 497/500\n",
      "90/90 [==============================] - 0s - loss: 4.7076     \n",
      "Epoch 498/500\n",
      "90/90 [==============================] - 0s - loss: 4.3039     \n",
      "Epoch 499/500\n",
      "90/90 [==============================] - 0s - loss: 4.2393     \n",
      "Epoch 500/500\n",
      "90/90 [==============================] - 0s - loss: 4.7988     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "if use_LSTM & lstm_stateful:\n",
    "        \n",
    "    for i in range(num_epochs):\n",
    "            print('epoch: ' + str(i))\n",
    "            # shuffle must be False!\n",
    "            model.fit(X_train, y_train, epochs = 1, batch_size = batch_size, shuffle = False)\n",
    "            model.reset_states()\n",
    "            \n",
    "else: \n",
    "    model.fit(X_train, y_train, epochs = num_epochs, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/10 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14562789704650642"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss = np.nan\n",
    "if lstm_stateful:\n",
    "    test_loss = model.evaluate(X_test, y_test, batch_size = batch_size)\n",
    "else:\n",
    "    test_loss = model.evaluate(X_test, y_test, batch_size = batch_size)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lstm_stateful:\n",
    "    model.reset_states()\n",
    "    pred_train = model.predict(X_train, batch_size = batch_size)\n",
    "    model.reset_states()\n",
    "    pred_test = model.predict(X_test, batch_size = batch_size)\n",
    "else:\n",
    "    pred_train = model.predict(X_train, batch_size = batch_size)\n",
    "    pred_test = model.predict(X_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 26.04081154]\n",
      "  [ 27.08862877]\n",
      "  [ 28.02630615]\n",
      "  [ 29.15218353]\n",
      "  [ 30.31283951]]]\n",
      "[[[ 27.14068222]\n",
      "  [ 28.11089897]\n",
      "  [ 29.06536865]\n",
      "  [ 30.18508148]\n",
      "  [ 31.34586906]]]\n",
      "[[[ 28.25746918]\n",
      "  [ 29.14868546]\n",
      "  [ 30.1166153 ]\n",
      "  [ 31.22247887]\n",
      "  [ 32.37998962]]]\n",
      "[[[ 29.3720665 ]\n",
      "  [ 30.19871902]\n",
      "  [ 31.17583466]\n",
      "  [ 32.26210785]\n",
      "  [ 33.41367722]]]\n",
      "[[[ 30.4692421 ]\n",
      "  [ 31.25772476]\n",
      "  [ 32.2390976 ]\n",
      "  [ 33.3019371 ]\n",
      "  [ 34.44534302]]]\n",
      "[[[ 31.53915024]\n",
      "  [ 32.32231903]\n",
      "  [ 33.30286789]\n",
      "  [ 34.34003067]\n",
      "  [ 35.47330475]]]\n",
      "[[[ 32.57761383]\n",
      "  [ 33.38915253]\n",
      "  [ 34.36410522]\n",
      "  [ 35.37451935]\n",
      "  [ 36.49593353]]]\n",
      "[[[ 33.58545685]\n",
      "  [ 34.45514679]\n",
      "  [ 35.42034912]\n",
      "  [ 36.40371704]\n",
      "  [ 37.51177979]]]\n",
      "[[[ 34.56735992]\n",
      "  [ 35.51779556]\n",
      "  [ 36.46977997]\n",
      "  [ 37.42622375]\n",
      "  [ 38.51980209]]]\n",
      "[[[ 35.53060532]\n",
      "  [ 36.57525635]\n",
      "  [ 37.51119232]\n",
      "  [ 38.44110489]\n",
      "  [ 39.51946259]]]\n"
     ]
    }
   ],
   "source": [
    "for i in X_test:\n",
    "    if lstm_stateful:\n",
    "        model.reset_states()\n",
    "    #print(i)\n",
    "    r = i.reshape(1, len(i), 1)\n",
    "    #print(i.shape), print(r.shape)\n",
    "    print(model.predict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 26.04081154],\n",
       "        [ 27.08862877],\n",
       "        [ 28.02630615],\n",
       "        [ 29.15218353],\n",
       "        [ 30.31283951]],\n",
       "\n",
       "       [[ 27.14068222],\n",
       "        [ 28.11089897],\n",
       "        [ 29.06536865],\n",
       "        [ 30.18508148],\n",
       "        [ 31.34586906]],\n",
       "\n",
       "       [[ 28.25746918],\n",
       "        [ 29.14868546],\n",
       "        [ 30.1166153 ],\n",
       "        [ 31.22247887],\n",
       "        [ 32.37998962]],\n",
       "\n",
       "       [[ 29.3720665 ],\n",
       "        [ 30.19871902],\n",
       "        [ 31.17583466],\n",
       "        [ 32.26210785],\n",
       "        [ 33.41367722]],\n",
       "\n",
       "       [[ 30.4692421 ],\n",
       "        [ 31.25772476],\n",
       "        [ 32.2390976 ],\n",
       "        [ 33.3019371 ],\n",
       "        [ 34.44534302]],\n",
       "\n",
       "       [[ 31.53915024],\n",
       "        [ 32.32231903],\n",
       "        [ 33.30286789],\n",
       "        [ 34.34003067],\n",
       "        [ 35.47330475]],\n",
       "\n",
       "       [[ 32.57761383],\n",
       "        [ 33.38915253],\n",
       "        [ 34.36410522],\n",
       "        [ 35.37451935],\n",
       "        [ 36.49593353]],\n",
       "\n",
       "       [[ 33.58545685],\n",
       "        [ 34.45514679],\n",
       "        [ 35.42034912],\n",
       "        [ 36.40371704],\n",
       "        [ 37.51177979]],\n",
       "\n",
       "       [[ 34.56735992],\n",
       "        [ 35.51779556],\n",
       "        [ 36.46977997],\n",
       "        [ 37.42622375],\n",
       "        [ 38.51980209]],\n",
       "\n",
       "       [[ 35.53060532],\n",
       "        [ 36.57525635],\n",
       "        [ 37.51119232],\n",
       "        [ 38.44110489],\n",
       "        [ 39.51946259]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dependent_predictions(model, data, prediction_window):\n",
    "    prediction_seqs = []\n",
    "    for i in range(int(len(data)/prediction_window)):\n",
    "        print('Calculating predictions starting from: {}'.format(i))\n",
    "        curr_frame = data[i*prediction_window]\n",
    "        predicted = []\n",
    "        for j in range(prediction_window):\n",
    "            #print('Calculating single prediction: {}'.format(j))\n",
    "            #print(curr_frame)\n",
    "            pred = model.predict(curr_frame[np.newaxis,:,:])[0,0]\n",
    "            #pred = model.predict(curr_frame.reshape(1, len(curr_frame), 1)) # same\n",
    "            #print(pred)\n",
    "            predicted.append(pred)\n",
    "            curr_frame = curr_frame[1:] \n",
    "            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
    "        prediction_seqs.append(predicted)\n",
    "    return prediction_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating predictions starting from: 0\n",
      "Calculating predictions starting from: 1\n",
      "Calculating predictions starting from: 2\n",
      "Calculating predictions starting from: 3\n",
      "Calculating predictions starting from: 4\n",
      "Calculating predictions starting from: 5\n",
      "Calculating predictions starting from: 6\n",
      "Calculating predictions starting from: 7\n",
      "Calculating predictions starting from: 8\n",
      "Calculating predictions starting from: 9\n",
      "Calculating predictions starting from: 10\n",
      "Calculating predictions starting from: 11\n",
      "Calculating predictions starting from: 12\n",
      "Calculating predictions starting from: 13\n",
      "Calculating predictions starting from: 14\n",
      "Calculating predictions starting from: 15\n",
      "Calculating predictions starting from: 16\n",
      "Calculating predictions starting from: 17\n",
      "Calculating predictions starting from: 0\n",
      "Calculating predictions starting from: 1\n"
     ]
    }
   ],
   "source": [
    "prediction_window = 5 \n",
    "\n",
    "prediction_seqs_train = calc_dependent_predictions(model, X_train, prediction_window)\n",
    "prediction_seqs_test = calc_dependent_predictions(model, X_test, prediction_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([ 10.6394825], dtype=float32),\n",
       "  array([ 10.20870495], dtype=float32),\n",
       "  array([ 9.90982914], dtype=float32),\n",
       "  array([ 9.79043388], dtype=float32),\n",
       "  array([ 9.91140556], dtype=float32)],\n",
       " [array([ 10.3433857], dtype=float32),\n",
       "  array([ 11.14436531], dtype=float32),\n",
       "  array([ 12.31319618], dtype=float32),\n",
       "  array([ 13.74533844], dtype=float32),\n",
       "  array([ 15.25131226], dtype=float32)],\n",
       " [array([ 16.65025711], dtype=float32),\n",
       "  array([ 17.85232544], dtype=float32),\n",
       "  array([ 18.86245155], dtype=float32),\n",
       "  array([ 19.73625946], dtype=float32),\n",
       "  array([ 20.54011536], dtype=float32)],\n",
       " [array([ 21.33127594], dtype=float32),\n",
       "  array([ 22.15166855], dtype=float32),\n",
       "  array([ 23.02736092], dtype=float32),\n",
       "  array([ 23.96984863], dtype=float32),\n",
       "  array([ 24.97801399], dtype=float32)],\n",
       " [array([ 26.04081154], dtype=float32),\n",
       "  array([ 27.14068222], dtype=float32),\n",
       "  array([ 28.25746918], dtype=float32),\n",
       "  array([ 29.3720665], dtype=float32),\n",
       "  array([ 30.4692421], dtype=float32)],\n",
       " [array([ 31.53915024], dtype=float32),\n",
       "  array([ 32.57761383], dtype=float32),\n",
       "  array([ 33.58545685], dtype=float32),\n",
       "  array([ 34.56735992], dtype=float32),\n",
       "  array([ 35.53060532], dtype=float32)],\n",
       " [array([ 36.48394012], dtype=float32),\n",
       "  array([ 37.4366188], dtype=float32),\n",
       "  array([ 38.39772797], dtype=float32),\n",
       "  array([ 39.37566376], dtype=float32),\n",
       "  array([ 40.37771606], dtype=float32)],\n",
       " [array([ 41.36795044], dtype=float32),\n",
       "  array([ 42.35676193], dtype=float32),\n",
       "  array([ 43.38221741], dtype=float32),\n",
       "  array([ 44.4449234], dtype=float32),\n",
       "  array([ 45.54354858], dtype=float32)],\n",
       " [array([ 46.67488861], dtype=float32),\n",
       "  array([ 47.83412933], dtype=float32),\n",
       "  array([ 49.01511383], dtype=float32),\n",
       "  array([ 50.21072388], dtype=float32),\n",
       "  array([ 51.41335297], dtype=float32)],\n",
       " [array([ 52.61534882], dtype=float32),\n",
       "  array([ 53.80941772], dtype=float32),\n",
       "  array([ 54.9889946], dtype=float32),\n",
       "  array([ 56.14849091], dtype=float32),\n",
       "  array([ 57.28341675], dtype=float32)],\n",
       " [array([ 58.39045715], dtype=float32),\n",
       "  array([ 59.46741486], dtype=float32),\n",
       "  array([ 60.51312256], dtype=float32),\n",
       "  array([ 61.52727509], dtype=float32),\n",
       "  array([ 62.51030731], dtype=float32)],\n",
       " [array([ 63.46318817], dtype=float32),\n",
       "  array([ 64.38731384], dtype=float32),\n",
       "  array([ 65.28437042], dtype=float32),\n",
       "  array([ 66.15618134], dtype=float32),\n",
       "  array([ 67.00662994], dtype=float32)],\n",
       " [array([ 67.8368988], dtype=float32),\n",
       "  array([ 68.64736176], dtype=float32),\n",
       "  array([ 69.43982697], dtype=float32),\n",
       "  array([ 70.21605682], dtype=float32),\n",
       "  array([ 70.97763824], dtype=float32)],\n",
       " [array([ 71.72608185], dtype=float32),\n",
       "  array([ 72.46276093], dtype=float32),\n",
       "  array([ 73.18894958], dtype=float32),\n",
       "  array([ 73.90579224], dtype=float32),\n",
       "  array([ 74.61431885], dtype=float32)],\n",
       " [array([ 75.31548309], dtype=float32),\n",
       "  array([ 75.97612], dtype=float32),\n",
       "  array([ 76.61410522], dtype=float32),\n",
       "  array([ 77.24655151], dtype=float32),\n",
       "  array([ 77.84440613], dtype=float32)],\n",
       " [array([ 78.07592773], dtype=float32),\n",
       "  array([ 78.14032745], dtype=float32),\n",
       "  array([ 78.20052338], dtype=float32),\n",
       "  array([ 78.2569809], dtype=float32),\n",
       "  array([ 78.31014252], dtype=float32)],\n",
       " [array([ 78.36037445], dtype=float32),\n",
       "  array([ 78.40801239], dtype=float32),\n",
       "  array([ 78.45335388], dtype=float32),\n",
       "  array([ 78.49665833], dtype=float32),\n",
       "  array([ 78.5209198], dtype=float32)],\n",
       " [array([ 78.53355408], dtype=float32),\n",
       "  array([ 78.54476166], dtype=float32),\n",
       "  array([ 78.55472565], dtype=float32),\n",
       "  array([ 78.56355286], dtype=float32),\n",
       "  array([ 78.57139587], dtype=float32)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_seqs_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([ 26.04081154], dtype=float32),\n",
       "  array([ 27.14068222], dtype=float32),\n",
       "  array([ 28.25746918], dtype=float32),\n",
       "  array([ 29.3720665], dtype=float32),\n",
       "  array([ 30.4692421], dtype=float32)],\n",
       " [array([ 31.53915024], dtype=float32),\n",
       "  array([ 32.57761383], dtype=float32),\n",
       "  array([ 33.58545685], dtype=float32),\n",
       "  array([ 34.56735992], dtype=float32),\n",
       "  array([ 35.53060532], dtype=float32)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_seqs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  6.],\n",
       "        [  7.],\n",
       "        [  8.],\n",
       "        [  9.],\n",
       "        [ 10.]],\n",
       "\n",
       "       [[  7.],\n",
       "        [  8.],\n",
       "        [  9.],\n",
       "        [ 10.],\n",
       "        [ 11.]],\n",
       "\n",
       "       [[  8.],\n",
       "        [  9.],\n",
       "        [ 10.],\n",
       "        [ 11.],\n",
       "        [ 12.]],\n",
       "\n",
       "       [[  9.],\n",
       "        [ 10.],\n",
       "        [ 11.],\n",
       "        [ 12.],\n",
       "        [ 13.]],\n",
       "\n",
       "       [[ 10.],\n",
       "        [ 11.],\n",
       "        [ 12.],\n",
       "        [ 13.],\n",
       "        [ 14.]],\n",
       "\n",
       "       [[ 11.],\n",
       "        [ 12.],\n",
       "        [ 13.],\n",
       "        [ 14.],\n",
       "        [ 15.]],\n",
       "\n",
       "       [[ 12.],\n",
       "        [ 13.],\n",
       "        [ 14.],\n",
       "        [ 15.],\n",
       "        [ 16.]],\n",
       "\n",
       "       [[ 13.],\n",
       "        [ 14.],\n",
       "        [ 15.],\n",
       "        [ 16.],\n",
       "        [ 17.]],\n",
       "\n",
       "       [[ 14.],\n",
       "        [ 15.],\n",
       "        [ 16.],\n",
       "        [ 17.],\n",
       "        [ 18.]],\n",
       "\n",
       "       [[ 15.],\n",
       "        [ 16.],\n",
       "        [ 17.],\n",
       "        [ 18.],\n",
       "        [ 19.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.6394825 ],\n",
       "       [ 10.20870495],\n",
       "       [  9.90982914],\n",
       "       [  9.79043388],\n",
       "       [  9.91140556],\n",
       "       [ 10.3433857 ],\n",
       "       [ 11.14436531],\n",
       "       [ 12.31319618],\n",
       "       [ 13.74533844],\n",
       "       [ 15.25131226]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train[:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 26.],\n",
       "        [ 27.],\n",
       "        [ 28.],\n",
       "        [ 29.],\n",
       "        [ 30.]],\n",
       "\n",
       "       [[ 27.],\n",
       "        [ 28.],\n",
       "        [ 29.],\n",
       "        [ 30.],\n",
       "        [ 31.]],\n",
       "\n",
       "       [[ 28.],\n",
       "        [ 29.],\n",
       "        [ 30.],\n",
       "        [ 31.],\n",
       "        [ 32.]],\n",
       "\n",
       "       [[ 29.],\n",
       "        [ 30.],\n",
       "        [ 31.],\n",
       "        [ 32.],\n",
       "        [ 33.]],\n",
       "\n",
       "       [[ 30.],\n",
       "        [ 31.],\n",
       "        [ 32.],\n",
       "        [ 33.],\n",
       "        [ 34.]],\n",
       "\n",
       "       [[ 31.],\n",
       "        [ 32.],\n",
       "        [ 33.],\n",
       "        [ 34.],\n",
       "        [ 35.]],\n",
       "\n",
       "       [[ 32.],\n",
       "        [ 33.],\n",
       "        [ 34.],\n",
       "        [ 35.],\n",
       "        [ 36.]],\n",
       "\n",
       "       [[ 33.],\n",
       "        [ 34.],\n",
       "        [ 35.],\n",
       "        [ 36.],\n",
       "        [ 37.]],\n",
       "\n",
       "       [[ 34.],\n",
       "        [ 35.],\n",
       "        [ 36.],\n",
       "        [ 37.],\n",
       "        [ 38.]],\n",
       "\n",
       "       [[ 35.],\n",
       "        [ 36.],\n",
       "        [ 37.],\n",
       "        [ 38.],\n",
       "        [ 39.]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 26.04081154],\n",
       "       [ 27.14068222],\n",
       "       [ 28.25746918],\n",
       "       [ 29.3720665 ],\n",
       "       [ 30.4692421 ],\n",
       "       [ 31.53915024],\n",
       "       [ 32.57761383],\n",
       "       [ 33.58545685],\n",
       "       [ 34.56735992],\n",
       "       [ 35.53060532]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test[:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if scale:\n",
    "    pred_train = scaler.inverse_transform(pred_train)\n",
    "    y_train = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "    pred_test = scaler.inverse_transform(pred_test)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[  6.],\n",
       "         [  7.],\n",
       "         [  8.],\n",
       "         [  9.],\n",
       "         [ 10.]],\n",
       " \n",
       "        [[  7.],\n",
       "         [  8.],\n",
       "         [  9.],\n",
       "         [ 10.],\n",
       "         [ 11.]],\n",
       " \n",
       "        [[  8.],\n",
       "         [  9.],\n",
       "         [ 10.],\n",
       "         [ 11.],\n",
       "         [ 12.]],\n",
       " \n",
       "        [[  9.],\n",
       "         [ 10.],\n",
       "         [ 11.],\n",
       "         [ 12.],\n",
       "         [ 13.]],\n",
       " \n",
       "        [[ 10.],\n",
       "         [ 11.],\n",
       "         [ 12.],\n",
       "         [ 13.],\n",
       "         [ 14.]],\n",
       " \n",
       "        [[ 11.],\n",
       "         [ 12.],\n",
       "         [ 13.],\n",
       "         [ 14.],\n",
       "         [ 15.]],\n",
       " \n",
       "        [[ 12.],\n",
       "         [ 13.],\n",
       "         [ 14.],\n",
       "         [ 15.],\n",
       "         [ 16.]],\n",
       " \n",
       "        [[ 13.],\n",
       "         [ 14.],\n",
       "         [ 15.],\n",
       "         [ 16.],\n",
       "         [ 17.]],\n",
       " \n",
       "        [[ 14.],\n",
       "         [ 15.],\n",
       "         [ 16.],\n",
       "         [ 17.],\n",
       "         [ 18.]],\n",
       " \n",
       "        [[ 15.],\n",
       "         [ 16.],\n",
       "         [ 17.],\n",
       "         [ 18.],\n",
       "         [ 19.]]]), array([[ 10.6394825 ],\n",
       "        [ 10.20870495],\n",
       "        [  9.90982914],\n",
       "        [  9.79043388],\n",
       "        [  9.91140556],\n",
       "        [ 10.3433857 ],\n",
       "        [ 11.14436531],\n",
       "        [ 12.31319618],\n",
       "        [ 13.74533844],\n",
       "        [ 15.25131226]], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10],pred_train[:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 26.],\n",
       "         [ 27.],\n",
       "         [ 28.],\n",
       "         [ 29.],\n",
       "         [ 30.]],\n",
       " \n",
       "        [[ 27.],\n",
       "         [ 28.],\n",
       "         [ 29.],\n",
       "         [ 30.],\n",
       "         [ 31.]],\n",
       " \n",
       "        [[ 28.],\n",
       "         [ 29.],\n",
       "         [ 30.],\n",
       "         [ 31.],\n",
       "         [ 32.]],\n",
       " \n",
       "        [[ 29.],\n",
       "         [ 30.],\n",
       "         [ 31.],\n",
       "         [ 32.],\n",
       "         [ 33.]],\n",
       " \n",
       "        [[ 30.],\n",
       "         [ 31.],\n",
       "         [ 32.],\n",
       "         [ 33.],\n",
       "         [ 34.]],\n",
       " \n",
       "        [[ 31.],\n",
       "         [ 32.],\n",
       "         [ 33.],\n",
       "         [ 34.],\n",
       "         [ 35.]],\n",
       " \n",
       "        [[ 32.],\n",
       "         [ 33.],\n",
       "         [ 34.],\n",
       "         [ 35.],\n",
       "         [ 36.]],\n",
       " \n",
       "        [[ 33.],\n",
       "         [ 34.],\n",
       "         [ 35.],\n",
       "         [ 36.],\n",
       "         [ 37.]],\n",
       " \n",
       "        [[ 34.],\n",
       "         [ 35.],\n",
       "         [ 36.],\n",
       "         [ 37.],\n",
       "         [ 38.]],\n",
       " \n",
       "        [[ 35.],\n",
       "         [ 36.],\n",
       "         [ 37.],\n",
       "         [ 38.],\n",
       "         [ 39.]]]), array([[ 26.04081154],\n",
       "        [ 27.14068222],\n",
       "        [ 28.25746918],\n",
       "        [ 29.3720665 ],\n",
       "        [ 30.4692421 ],\n",
       "        [ 31.53915024],\n",
       "        [ 32.57761383],\n",
       "        [ 33.58545685],\n",
       "        [ 34.56735992],\n",
       "        [ 35.53060532]], dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:],pred_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-8c817673a9be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate root mean squared error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrsme_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Score: %.2f RMSE'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrsme_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrsme_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Score: %.2f RMSE'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrsme_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/key/software/anaconda3/envs/tf3.5/lib/python3.5/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 231\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    232\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n\u001b[1;32m    233\u001b[0m                                weights=sample_weight)\n",
      "\u001b[0;32m/home/key/software/anaconda3/envs/tf3.5/lib/python3.5/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/key/software/anaconda3/envs/tf3.5/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 405\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "rsme_train = math.sqrt(mean_squared_error(y_train, pred_train[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (rsme_train))\n",
    "rsme_test = math.sqrt(mean_squared_error(y_test, pred_test[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (rsme_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(ts_train), len(pred_train), len(y_train))\n",
    "len(ts_test), len(pred_test), len(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "pred_train_shifted = np.empty_like(ts_all)\n",
    "print(pred_train_shifted.size)\n",
    "pred_train_shifted[:, :] = np.nan\n",
    "# train predictions start at position window_size + 1 (or window_size, if counting from 0)\n",
    "pred_train_shifted[window_size : len(pred_train) + window_size, :] = pred_train\n",
    "pred_train_shifted[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shift test predictions for plotting\n",
    "window_size = lstm_num_timesteps if use_LSTM else mlp_num_features\n",
    "pred_test_shifted = np.empty_like(ts_all)\n",
    "pred_test_shifted[:, :] = np.nan\n",
    "pred_test_shifted[len(pred_train) + (window_size * 2) : len_overall + 1, :] = pred_test\n",
    "pred_test_shifted[-13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ts_all)\n",
    "plt.plot(pred_train_shifted)\n",
    "plt.plot(pred_test_shifted)\n",
    "plt.savefig(testname + '_lstm_' + str(use_LSTM) + '_stateful_' + str(lstm_stateful) + '_window_' + str(window_size) +\n",
    "            '_epochs_' + str(num_epochs) + '_2layers_' + str(lstm_stack_layers) + '_scale_' + str(scale) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_start = -30\n",
    "plot_end = -1\n",
    "plt.plot(ts_all[plot_start:plot_end])\n",
    "plt.plot(pred_train_shifted[plot_start:plot_end])\n",
    "plt.plot(pred_test_shifted[plot_start:plot_end])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(predicted_data, true_data):\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    plt.plot(predicted_data, label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results(pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results(pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results_multiple(predicted_data, true_data, prediction_window):\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    #Pad the list of predictions to shift it in the graph to it's correct start\n",
    "    for i, data in enumerate(predicted_data):\n",
    "        padding = [None for p in range(i * prediction_window)]\n",
    "        plt.plot(padding + data, label='Prediction')\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results_multiple(prediction_seqs_train, y_test, prediction_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results_multiple(prediction_seqs_test, y_test, prediction_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seq2seq\n",
    "# predict and append prediction to existing values\n",
    "# TimeDistributedDense after return_sequences True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://groups.google.com/forum/#!topic/keras-users/9GsDwkSdqBg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
